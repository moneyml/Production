{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from nlp_utils import clean_text, pos_tag_text\n",
    "sys.path.append(\"../\")\n",
    "from param_config import config\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.float_format',lambda x: '%.5f'%x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Load data...\")\n",
    "\n",
    "dfTrain = pd.read_csv(config.original_train_data_path)\n",
    "\n",
    "dfPred= pd.read_csv(config.original_test_data_path)\n",
    "dfPred2= pd.read_csv(config.original_test_data2_path)\n",
    "# number of train/test samples\n",
    "num_train, num_pred = dfTrain.shape[0], dfPred.shape[0]\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "f = open('../../Data/col_name.txt','r')  \n",
    "a = f.read()  \n",
    "col_name = eval(a)  \n",
    "f.close() \n",
    "\n",
    "f = open('../../Data/Procedure.txt','r')  \n",
    "a = f.read()  \n",
    "procedure = eval(a)  \n",
    "f.close()\n",
    "\n",
    "f = open('../../Cache/var_change.txt','r')\n",
    "a = f.read()\n",
    "var_change = eval(a)\n",
    "f.close()\n",
    "\n",
    "dfTrain = dfTrain.rename(columns=col_name)\n",
    "raw_predictors = dfTrain.columns.tolist()[1:-1]\n",
    "dfTrain = dfTrain.set_index(dfTrain['ID'])\n",
    "dfPred = dfPred.rename(columns=col_name)\n",
    "dfPred = dfPred.set_index(dfPred['ID'])\n",
    "dfPred2 = dfPred2.rename(columns=col_name)\n",
    "\n",
    "dfPredAll = pd.concat([dfPred,dfPred2],axis=0)\n",
    "dfPredAll = dfPredAll.set_index(dfPredAll['ID'])\n",
    "\n",
    "dfAll = pd.concat([dfTrain,dfPred,dfPred2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of NAN\n",
    "feat_na = ['Total_NAN_Num']\n",
    "dfTrain['Total_NAN_Num']=dfTrain[raw_predictors].T.isnull().sum().tolist()\n",
    "dfPred['Total_NAN_Num']=dfPred[raw_predictors].T.isnull().sum().tolist()\n",
    "for key,pro in procedure.items():\n",
    "    tmp_pro = []\n",
    "    for var in pro:\n",
    "        if var in dfTrain.columns:\n",
    "            tmp_pro.append(var)\n",
    "    dfTrain[key+'_NAN_Num']=dfTrain[tmp_pro].T.isnull().sum().tolist()\n",
    "    dfPred[key+'_NAN_Num']=dfPred[tmp_pro].T.isnull().sum().tolist()\n",
    "    feat_na.append(key+'_NAN_Num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load features\n",
    "Feat_file = ['Feat_cnt_col','Feat_cnt_tool','Feat_pcent_col','Feat_pcent_tool']\n",
    "ex_feat = []\n",
    "feat_for_ensemble = [raw_predictors]\n",
    "for filename in Feat_file:\n",
    "    tmp = pd.read_csv('../../Cache/0_to_nan/'+filename+'.csv')\n",
    "    tmp.set_index(tmp['ID'],inplace=True,drop=True)\n",
    "    tmp_feat_list = tmp.columns.tolist()\n",
    "    if 'ID' in tmp_feat_list:\n",
    "        tmp_feat_list.remove('ID')\n",
    "    ex_feat += tmp_feat_list\n",
    "    feat_for_ensemble.append(tmp_feat_list)\n",
    "    dfTrain = pd.merge(dfTrain,tmp,'left','ID')\n",
    "    dfPred = pd.merge(dfPred,tmp,'left','ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define the variables\n",
    "'''var_change = {'raw':raw}\n",
    "for filename in Feat_file:\n",
    "    tmp = pd.read_csv('../../Cache/'+filename+'.csv',nrows=10)\n",
    "    tmp.set_index(tmp['ID'],inplace=True,drop=True)\n",
    "    tmp_feat_list = tmp.columns.tolist()\n",
    "    if 'ID' in tmp_feat_list:\n",
    "        tmp_feat_list.remove('ID')\n",
    "    var_change[filename] = {'constant':[],'category':[]}\n",
    "    for var in tmp_feat_list:\n",
    "        if dfTrain[var].nunique()==1 and dfTrain[var].isnull().sum()==0:\n",
    "            var_change[filename]['constant'].append(var)\n",
    "        elif dfTrain[var].nunique()<=5 or 'TOOL' in var:\n",
    "            var_change[filename]['category'].append(var)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw\n",
      "Feat_cnt_col\n",
      "Feat_cnt_tool\n",
      "Feat_pcent_col\n",
      "Feat_pcent_tool\n"
     ]
    }
   ],
   "source": [
    "#define the input variables, dummies\n",
    "predictors = raw_predictors+ex_feat+feat_na\n",
    "for key,value in var_change.items():\n",
    "    print(key)\n",
    "    if key!='raw':\n",
    "        continue\n",
    "    for var in var_change[key]['constant']:\n",
    "        if not var in predictors:\n",
    "            continue\n",
    "        predictors.remove(var)\n",
    "    for var in var_change[key]['category']:\n",
    "        if not var in predictors:\n",
    "            continue\n",
    "        predictors.remove(var)\n",
    "        tmpTrain = pd.get_dummies(dfTrain[var],prefix=var,dummy_na=True)\n",
    "        tmpPred = pd.get_dummies(dfPred[var],prefix=var,dummy_na=True)\n",
    "        predictors = predictors + tmpTrain.columns.tolist()\n",
    "        dfTrain = pd.concat([dfTrain,tmpTrain],axis=1)\n",
    "        dfPred = pd.concat([dfPred,tmpPred],axis=1)\n",
    "for var in predictors:\n",
    "    if var not in dfPred.columns:\n",
    "        dfPred[var] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_kfold(dfTrain,dfPred,predictors,n_splits=5,param={'max_depth':4, 'eta':0.01, 'silent':0,'objective':'reg:linear','lambda':1.5,'subsample':0.8,\n",
    "                         'colsample_bytree':0.8}):  \n",
    "    kf = KFold(n_splits=n_splits,shuffle=True)\n",
    "    dpred = xgb.DMatrix(dfPred[predictors].values,label=[0]*len(dfPred),missing=np.nan)\n",
    "    imp = pd.DataFrame({'variable':predictors,'lk':['f'+str(i) for i in range(len(predictors))]})\n",
    "    round=0\n",
    "    for train_index, test_index in kf.split(dfTrain):\n",
    "        round+=1\n",
    "        train_X = dfTrain.loc[train_index,predictors]\n",
    "        test_X = dfTrain.loc[test_index,predictors]\n",
    "        train_Y = dfTrain.loc[train_index,'Y']\n",
    "        test_Y = dfTrain.loc[test_index,'Y']\n",
    "\n",
    "        dtrain = xgb.DMatrix(train_X.values, label=train_Y.values, missing = np.nan)\n",
    "        dtest = xgb.DMatrix(test_X.values, label=test_Y.values, missing = np.nan)\n",
    "        param  = param\n",
    "        evallist  = [(dtrain,'train'),(dtest,'eval')]  \n",
    "        num_round = 5000\n",
    "        evals_dict = {}\n",
    "        model = xgb.train(param,dtrain,num_round, evallist,early_stopping_rounds=50,evals_result=evals_dict,verbose_eval =50)\n",
    "        performance_df = pd.DataFrame(evals_dict['eval'])\n",
    "        bst_tree = len(performance_df)-51\n",
    "        pred_test = model.predict(dtest,ntree_limit =bst_tree)\n",
    "\n",
    "        tmp_imp = pd.DataFrame(model.get_score(importance_type='gain'),index=['imp_fold%d'%round]).T\n",
    "        tmp_imp['lk'] = tmp_imp.index\n",
    "        imp = imp.merge(tmp_imp,'left','lk').fillna(0)\n",
    "\n",
    "\n",
    "        pred_score = model.predict(dpred,ntree_limit =bst_tree)\n",
    "        if round==1:\n",
    "            test_result = pd.DataFrame({'ID':dfTrain.loc[test_index,'ID'].values,'score':pred_test,'target':test_Y})\n",
    "            result = pd.DataFrame({'ID':dfPred['ID'],'Score_%d'%round:pred_score})\n",
    "        else:\n",
    "            test_result = pd.concat([test_result,pd.DataFrame({'ID':dfTrain.loc[test_index,'ID'].values,'score':pred_test,'target':test_Y})],axis=0)\n",
    "            result = result.merge(pd.DataFrame({'ID':dfPred['ID'],'Score_%d'%round:pred_score}),'inner','ID')\n",
    "    print(\"Test MSE:\",metrics.mean_squared_error(test_result['target'], test_result['score']))\n",
    "    return test_result,result,imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.33667\teval-rmse:2.31066\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:1.42636\teval-rmse:1.40045\n",
      "[100]\ttrain-rmse:0.878988\teval-rmse:0.856285\n",
      "[150]\ttrain-rmse:0.551515\teval-rmse:0.53809\n",
      "[200]\ttrain-rmse:0.353\teval-rmse:0.354282\n",
      "[250]\ttrain-rmse:0.232232\teval-rmse:0.255504\n",
      "[300]\ttrain-rmse:0.158064\teval-rmse:0.206207\n",
      "[350]\ttrain-rmse:0.112462\teval-rmse:0.184387\n",
      "[400]\ttrain-rmse:0.084146\teval-rmse:0.17602\n",
      "[450]\ttrain-rmse:0.065925\teval-rmse:0.172613\n",
      "[500]\ttrain-rmse:0.054333\teval-rmse:0.171836\n",
      "[550]\ttrain-rmse:0.045453\teval-rmse:0.171942\n",
      "Stopping. Best iteration:\n",
      "[510]\ttrain-rmse:0.052232\teval-rmse:0.171798\n",
      "\n",
      "[0]\ttrain-rmse:2.3271\teval-rmse:2.34962\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:1.42043\teval-rmse:1.44248\n",
      "[100]\ttrain-rmse:0.876049\teval-rmse:0.89809\n",
      "[150]\ttrain-rmse:0.549542\teval-rmse:0.576823\n",
      "[200]\ttrain-rmse:0.351797\teval-rmse:0.388023\n",
      "[250]\ttrain-rmse:0.231708\teval-rmse:0.283321\n",
      "[300]\ttrain-rmse:0.158436\teval-rmse:0.226444\n",
      "[350]\ttrain-rmse:0.11303\teval-rmse:0.198382\n",
      "[400]\ttrain-rmse:0.084554\teval-rmse:0.185424\n",
      "[450]\ttrain-rmse:0.066304\teval-rmse:0.179359\n",
      "[500]\ttrain-rmse:0.054034\teval-rmse:0.176006\n",
      "[550]\ttrain-rmse:0.045304\teval-rmse:0.174416\n",
      "[600]\ttrain-rmse:0.03852\teval-rmse:0.173807\n",
      "[650]\ttrain-rmse:0.032975\teval-rmse:0.173488\n",
      "[700]\ttrain-rmse:0.028639\teval-rmse:0.173398\n",
      "[750]\ttrain-rmse:0.024955\teval-rmse:0.173145\n",
      "[800]\ttrain-rmse:0.021695\teval-rmse:0.173041\n",
      "Stopping. Best iteration:\n",
      "[777]\ttrain-rmse:0.023207\teval-rmse:0.173009\n",
      "\n",
      "[0]\ttrain-rmse:2.33169\teval-rmse:2.33056\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:1.42226\teval-rmse:1.4223\n",
      "[100]\ttrain-rmse:0.875887\teval-rmse:0.877593\n",
      "[150]\ttrain-rmse:0.548632\teval-rmse:0.55436\n",
      "[200]\ttrain-rmse:0.350867\teval-rmse:0.368627\n",
      "[250]\ttrain-rmse:0.230708\teval-rmse:0.267951\n",
      "[300]\ttrain-rmse:0.157599\teval-rmse:0.219897\n",
      "[350]\ttrain-rmse:0.113269\teval-rmse:0.198529\n",
      "[400]\ttrain-rmse:0.086028\teval-rmse:0.190494\n",
      "[450]\ttrain-rmse:0.068707\teval-rmse:0.187651\n",
      "[500]\ttrain-rmse:0.056667\teval-rmse:0.186641\n",
      "[550]\ttrain-rmse:0.047665\teval-rmse:0.186132\n",
      "[600]\ttrain-rmse:0.040785\teval-rmse:0.185658\n",
      "[650]\ttrain-rmse:0.035429\teval-rmse:0.185459\n",
      "Stopping. Best iteration:\n",
      "[641]\ttrain-rmse:0.036406\teval-rmse:0.185415\n",
      "\n",
      "[0]\ttrain-rmse:2.32144\teval-rmse:2.37178\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:1.41674\teval-rmse:1.46703\n",
      "[100]\ttrain-rmse:0.872965\teval-rmse:0.924582\n",
      "[150]\ttrain-rmse:0.548186\teval-rmse:0.604847\n",
      "[200]\ttrain-rmse:0.352382\teval-rmse:0.415283\n",
      "[250]\ttrain-rmse:0.233218\teval-rmse:0.303476\n",
      "[300]\ttrain-rmse:0.160237\teval-rmse:0.240399\n",
      "[350]\ttrain-rmse:0.115793\teval-rmse:0.205095\n",
      "[400]\ttrain-rmse:0.088682\teval-rmse:0.186567\n",
      "[450]\ttrain-rmse:0.071101\teval-rmse:0.176003\n",
      "[500]\ttrain-rmse:0.058827\teval-rmse:0.17012\n",
      "[550]\ttrain-rmse:0.05003\teval-rmse:0.166885\n",
      "[600]\ttrain-rmse:0.042883\teval-rmse:0.164907\n",
      "[650]\ttrain-rmse:0.037277\teval-rmse:0.163464\n",
      "[700]\ttrain-rmse:0.032032\teval-rmse:0.162501\n",
      "[750]\ttrain-rmse:0.028044\teval-rmse:0.161869\n",
      "[800]\ttrain-rmse:0.024331\teval-rmse:0.161337\n",
      "[850]\ttrain-rmse:0.021383\teval-rmse:0.160945\n",
      "[900]\ttrain-rmse:0.018614\teval-rmse:0.160825\n",
      "[950]\ttrain-rmse:0.016317\teval-rmse:0.160541\n",
      "[1000]\ttrain-rmse:0.014266\teval-rmse:0.160601\n",
      "Stopping. Best iteration:\n",
      "[960]\ttrain-rmse:0.015862\teval-rmse:0.160531\n",
      "\n",
      "[0]\ttrain-rmse:2.34056\teval-rmse:2.29409\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:1.42873\teval-rmse:1.38101\n",
      "[100]\ttrain-rmse:0.880843\teval-rmse:0.83444\n",
      "[150]\ttrain-rmse:0.552669\teval-rmse:0.520062\n",
      "[200]\ttrain-rmse:0.354028\teval-rmse:0.336778\n",
      "[250]\ttrain-rmse:0.233771\teval-rmse:0.236549\n",
      "[300]\ttrain-rmse:0.160087\teval-rmse:0.186173\n",
      "[350]\ttrain-rmse:0.114445\teval-rmse:0.164528\n",
      "[400]\ttrain-rmse:0.086147\teval-rmse:0.156411\n",
      "[450]\ttrain-rmse:0.068032\teval-rmse:0.15395\n",
      "[500]\ttrain-rmse:0.05608\teval-rmse:0.153462\n",
      "[550]\ttrain-rmse:0.047351\teval-rmse:0.153266\n",
      "Stopping. Best iteration:\n",
      "[547]\ttrain-rmse:0.047725\teval-rmse:0.153257\n",
      "\n",
      "Test MSE: 0.0286267047194\n"
     ]
    }
   ],
   "source": [
    "test_result,result,imp = xgb_kfold(dfTrain,dfPred,predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ensemble experiment\n",
    "input_round = 0\n",
    "final_predictors = []\n",
    "tool_dummies = []\n",
    "for predictors in feat_for_ensemble:\n",
    "    input_round+=1\n",
    "    print(input_round)\n",
    "    if input_round==1:\n",
    "        for var in predictors:\n",
    "            if dfTrain[var].nunique()==1 and dfTrain[var].isnull().sum()==0:\n",
    "                predictors.remove(var)\n",
    "            elif dfTrain[var].nunique()<=5 or 'TOOL' in var:\n",
    "                predictors.remove(var)\n",
    "                tmpTrain = pd.get_dummies(dfTrain[var],prefix=var,dummy_na=True)\n",
    "                tmpPred = pd.get_dummies(dfPred[var],prefix=var,dummy_na=True)\n",
    "                predictors = predictors + tmpTrain.columns.tolist()\n",
    "                dfTrain = pd.concat([dfTrain,tmpTrain],axis=1)\n",
    "                dfPred = pd.concat([dfPred,tmpPred],axis=1)\n",
    "                if 'TOOL' in var:\n",
    "                    tool_dummies += tmpTrain.columns.tolist()\n",
    "    else:\n",
    "        predictors+=tool_dummies\n",
    "        '''for var in raw_predictors:\n",
    "            if 'TOOL' in var:\n",
    "                tmpTrain = pd.get_dummies(dfTrain[var],prefix=var,dummy_na=True)\n",
    "                tmpPred = pd.get_dummies(dfPred[var],prefix=var,dummy_na=True)\n",
    "                predictors = predictors + tmpTrain.columns.tolist()\n",
    "                dfTrain = pd.concat([dfTrain,tmpTrain],axis=1)\n",
    "                dfPred = pd.concat([dfPred,tmpPred],axis=1)'''             \n",
    "    for var in predictors:\n",
    "        if var not in dfPred.columns:\n",
    "            dfPred[var] = 0                      \n",
    "    ### model         \n",
    "    tmp_test_result,tmp_result,imp = xgb_kfold(dfTrain,dfPred,predictors)\n",
    "    \n",
    "    tmp_result['score']=tmp_result[['Score_1','Score_2','Score_3','Score_4','Score_5']].mean(axis=1)\n",
    "    tmp_result = tmp_result[['ID','score']]\n",
    "    \n",
    "    tmp_test_result = tmp_test_result.rename(columns={'score':'score%d'%input_round})\n",
    "    tmp_result = tmp_result.rename(columns={'score':'score%d'%input_round})\n",
    "    \n",
    "    final_predictors.append('score%d'%input_round)\n",
    "    \n",
    "    if input_round==1:\n",
    "        final_test_result = tmp_test_result.copy(deep=True)\n",
    "        final_result = tmp_result.copy(deep=True)\n",
    "    else:\n",
    "        final_test_result = final_test_result.merge(tmp_test_result[['ID','score%d'%input_round]],'inner','ID')\n",
    "        final_result = final_result.merge(tmp_result[['ID','score%d'%input_round]],'inner','ID')\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_test_result = final_test_result.merge(dfTrain[['ID','Y']],'inner','ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_result,result,imp = xgb_kfold(final_test_result,final_result,final_predictors,param={'max_depth':2, 'eta':0.01, 'silent':0,'objective':'reg:linear','lambda':1.5,'subsample':0.8,\n",
    "                         'colsample_bytree':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result['score']=result[['Score_1','Score_2','Score_3','Score_4','Score_5']].mean(axis=1)\n",
    "submit = result[['ID','score']]\n",
    "today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "submit.to_csv('../../Submission/submit_%s.csv'%today,header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_result.to_csv('../../Submission/test_version_%s.csv'%today,header=False,index=False)\n",
    "imp.to_csv('../../Submission/imp_%s.csv'%today,header=False,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
