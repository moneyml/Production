{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from nlp_utils import clean_text, pos_tag_text\n",
    "sys.path.append(\"../\")\n",
    "from param_config import config\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option('display.float_format',lambda x: '%.5f'%x)\n",
    "\n",
    "print(\"Load data...\")\n",
    "\n",
    "dfTrain = pd.read_csv(config.original_train_data_path)\n",
    "\n",
    "dfPred= pd.read_csv(config.original_test_data_path)\n",
    "dfPred2= pd.read_csv(config.original_test_data2_path)\n",
    "# number of train/test samples\n",
    "num_train, num_pred = dfTrain.shape[0], dfPred.shape[0]\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "f = open('../../Data/col_name.txt','r')  \n",
    "a = f.read()  \n",
    "col_name = eval(a)  \n",
    "f.close() \n",
    "\n",
    "f = open('../../Data/Procedure.txt','r')  \n",
    "a = f.read()  \n",
    "procedure = eval(a)  \n",
    "f.close() \n",
    "\n",
    "dfTrain = dfTrain.rename(columns=col_name)\n",
    "raw_predictors = dfTrain.columns.tolist()[1:-1]\n",
    "dfPred = dfPred.rename(columns=col_name)\n",
    "dfPred2 = dfPred2.rename(columns=col_name)\n",
    "\n",
    "dfPredAll = pd.concat([dfPred,dfPred2],axis=0)\n",
    "\n",
    "dfAll = pd.concat([dfTrain,dfPred,dfPred2],axis=0)\n",
    "\n",
    "#dfAll = dfAll.replace({'':np.nan,0:np.nan})\n",
    "dfAll = dfAll.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfAll = pd.read_csv('../../Data/All_non_missing_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfAll[pro_feat] = dfAll[pro_feat]-dfAll[pro_feat].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfAll = dfAll.replace({-99999999:-2.2222222})\n",
    "dfAll[['750X1450','750X1451']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfAll = dfAll.replace({np.nan:-99999999})\n",
    "pro_feat = [var for var in raw_predictors if not 'TOOL' in var ]\n",
    "corr = dfAll[pro_feat].corr()\n",
    "for i in range(len(corr)):\n",
    "    corr.iloc[i,i:] =np.nan\n",
    "print((corr==1.0).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#处理0这个比较异常的值,在trian set中有40条数据，在某些column上为0 ，需要处理，test_a中也有一条\n",
    "feature = ['ID']\n",
    "for var in raw_predictors:\n",
    "    if 'TOOL' in var:\n",
    "        feature.append(var)\n",
    "pick_list = (((corr==1.0).sum(axis=1))==0).values\n",
    "for i in range(len(pick_list)):\n",
    "    if pick_list[i]==True:\n",
    "        feature.append(corr.index[i])\n",
    "print(len(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#number of NAN\n",
    "dfAll['Total_NAN_Num']=dfAll[raw_predictors].T.isnull().sum().tolist()\n",
    "for key,pro in procedure.items():\n",
    "    dfAll[key+'_NAN_Num']=dfAll[pro].T.isnull().sum().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictors = dfAll.columns.tolist()[1:-13]\n",
    "raw_predictors[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count 计算特定值在变量中重复出现的次数'''\n",
    "dfAll = dfAll.replace({np.nan:-99999999})\n",
    "added = ['ID']\n",
    "for var in raw_predictors:\n",
    "    if not var in dfAll.columns:\n",
    "        continue\n",
    "    dfAll['cnt_'+var] = 0\n",
    "    added.append('cnt_'+var)\n",
    "    groups = dfAll.groupby([var])\n",
    "    for name,group in groups:\n",
    "        count = group[var].count()\n",
    "        dfAll['cnt_'+var].ix[group.index] = count\n",
    "dfAll[added].to_csv('../../Cache/Feat_cnt_col_nonmissing.csv',index=False)\n",
    "\n",
    "added = ['ID']\n",
    "for tool,pro in procedure.items():\n",
    "    if tool[-3:] in ['261','400','420']:\n",
    "        continue\n",
    "    groups = dfAll.groupby(['TOOL_'+tool[-3:]])\n",
    "    for var in pro:\n",
    "        if not var in dfAll.columns:\n",
    "            continue\n",
    "        dfAll['cnt_'+tool+'_'+var] = 0\n",
    "        added.append('cnt_'+tool+'_'+var)\n",
    "        for name,group in groups:\n",
    "            grps = group.groupby([var])\n",
    "            for name2,grp in grps:\n",
    "                dfAll['cnt_'+tool+'_'+var].ix[grp.index] = float(len(grp))/float(len(group))\n",
    "dfAll[added].to_csv('../../Cache/Feat_cnt_tool_nonmissing.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentile 计算某一个值在全集中的位置（百分制）\n",
    "dfAll = dfAll.replace({-99999999:np.nan})\n",
    "added = ['ID']\n",
    "for var in raw_predictors:\n",
    "    if not var in dfAll.columns:\n",
    "        continue\n",
    "    if 'TOOL' not in var:\n",
    "        dfAll['pcent_'+var] = dfAll[var].rank(method='max')/float(len(dfAll))\n",
    "        added.append('pcent_'+var)\n",
    "dfAll[added].to_csv('../../Cache/Feat_pcent_col_nonmissing.csv',index=False)\n",
    "t = 0\n",
    "added = ['ID']\n",
    "for tool,pro in procedure.items():\n",
    "    if tool[-3:] in ['261','400','420']:\n",
    "        continue\n",
    "    groups = dfAll.groupby(['TOOL_'+tool[-3:]]) \n",
    "    for var in pro:\n",
    "        if not var in dfAll.columns:\n",
    "            continue\n",
    "        dfAll['pcent_'+tool+'_'+var] = 0\n",
    "        added.append('pcent_'+tool+'_'+var)\n",
    "        for name,group in groups:\n",
    "            dfAll['pcent_'+tool+'_'+var].ix[group.index] = group[var].rank(method='max')/float(len(group))\n",
    "            \n",
    "dfAll[added].to_csv('../../Cache/Feat_pcent_tool_nonmissing.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_var = ['ID']\n",
    "for i in range(len(raw_predictors)-1):\n",
    "    for j in range(i+1,len(raw_predictors)):\n",
    "        if (dfAll[raw_predictors[i]].max()-dfAll[raw_predictors[j]].max())*(dfAll[raw_predictors[i]].min()-dfAll[raw_predictors[j]].min())<0:\n",
    "            if float(min(dfAll[raw_predictors[i]].max()-dfAll[raw_predictors[i]].min(),dfAll[raw_predictors[j]].max()-dfAll[raw_predictors[j]].min()))/max(dfAll[raw_predictors[i]].max()-dfAll[raw_predictors[i]].min(),dfAll[raw_predictors[j]].max()-dfAll[raw_predictors[j]].min())>=0.6:\n",
    "                tmp_var = raw_predictors[i]+'-'+raw_predictors[j]\n",
    "                dfAll[tmp_var] = dfAll[raw_predictors[i]]-dfAll[raw_predictors[j]]\n",
    "                minus_var.append(tmp_var)\n",
    "                print(tmp_var)\n",
    "        elif (dfAll[raw_predictors[i]].max()-dfAll[raw_predictors[j]].min())*(dfAll[raw_predictors[j]].max()-dfAll[raw_predictors[i]].min())>0:\n",
    "            if float(min(abs(dfAll[raw_predictors[i]].max()-dfAll[raw_predictors[j]].min()),abs(dfAll[raw_predictors[j]].max()-dfAll[raw_predictors[i]].min())))/max(abs(dfAll[raw_predictors[i]].max()-dfAll[raw_predictors[j]].min()),abs(dfAll[raw_predictors[j]].max()-dfAll[raw_predictors[i]].min()))>=0.6:\n",
    "                tmp_var = raw_predictors[i]+'-'+raw_predictors[j]\n",
    "                dfAll[tmp_var] = dfAll[raw_predictors[i]]-dfAll[raw_predictors[j]]\n",
    "                minus_var.append(tmp_var)\n",
    "                print(tmp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dffAll_minus = dfAll[minus_var]\n",
    "dffAll_minus.to_csv('../../../All_minus.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dffAll_minus.nunique()<=10).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added = ['ID']\n",
    "for var in minus_var[1:]:\n",
    "    if dffAll_minus[var].nunique()>10:\n",
    "        continue\n",
    "    dffAll_minus['cnt_'+var] = 0\n",
    "    added.append('cnt_'+var)\n",
    "    groups = dffAll_minus.groupby([var])\n",
    "    for name,group in groups:\n",
    "        count = group[var].count()\n",
    "        dffAll_minus['cnt_'+var].ix[group.index] = count\n",
    "for var in minus_var[1:]:\n",
    "    if 'TOOL' not in var:\n",
    "        dfAll['pcent_'+var] = dfAll[var].rank(method='max')/float(len(dfAll))\n",
    "        added.append('pcent_'+var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pivot 创建多组category variable 的两两交叉表,感觉只能用来计算分类型变量,好像现在直接可以用pandas解决\n",
    "def get_pivottable(X_train, X_test, use='all',Feat_list):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries, one per feature in the\n",
    "    basic data, containing cross-tabulated counts\n",
    "    for each column and each value of the feature.\n",
    "    \"\"\"\n",
    "    dictionaries = []\n",
    "    if use == 'all':\n",
    "        X = np.vstack((X_train, X_test))\n",
    "        filename = \"pivottable\"\n",
    "    elif use == 'train':\n",
    "        X = X_train\n",
    "        filename = \"pivottable_train\"\n",
    "    else:\n",
    "        X = X_test\n",
    "        filename = \"pivottable_test\"\n",
    "    for i in range(len(Feat_list)):\n",
    "        dictionaries.append({'total': 0})\n",
    "    try:\n",
    "        with open(\"cache/%s.pkl\" % filename, 'rb') as f:\n",
    "            logger.debug(\"loading cross-tabulated data from cache\")\n",
    "            dictionaries = pickle.load(f)\n",
    "    except IOError:\n",
    "        logger.debug(\"no cache found, cross-tabulating data\")\n",
    "        for i, row in enumerate(X):\n",
    "            for j in range(len(Feat_list)):\n",
    "                dictionaries[j]['total'] += 1\n",
    "                if row[j] not in dictionaries[j]:\n",
    "                    dictionaries[j][row[j]] = {'total': 1}\n",
    "                    for k, key in enumerate(Feat_list):\n",
    "                        dictionaries[j][row[j]][key] = {row[k]: 1}\n",
    "                else:\n",
    "                    dictionaries[j][row[j]]['total'] += 1\n",
    "                    for k, key in enumerate(Feat_list):\n",
    "                        if row[k] not in dictionaries[j][row[j]][key]:\n",
    "                            dictionaries[j][row[j]][key][row[k]] = 1\n",
    "                        else:\n",
    "                            dictionaries[j][row[j]][key][row[k]] += 1\n",
    "        with open(\"cache/%s.pkl\" % filename, 'wb') as f:\n",
    "            pickle.dump(dictionaries, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return dictionaries\n",
    "\n",
    "def creat_cate_corr(X_train, X_test, Feat_list=[var if 'TOOL' in var for var in raw_predictor]):\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    dictionaries = get_pivottable(X_train[Feat_list], X_test[Feat_list],Feat_list=Feat_list)\n",
    "    #dictionaries_train = get_pivottable(X_train[Feat_list], X_test[Feat_list], use='train',Feat_list=Feat_list)\n",
    "    #dictionaries_test = get_pivottable(X_test[Feat_list], X_test[Feat_list], use='test',Feat_list=Feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = pd.read_csv('../../Data/All_Data_dedup.csv')\n",
    "feat_needed = dfAll.columns.tolist()[14:]\n",
    "'''gap_list = []\n",
    "for i in range(len(feat_needed)-1):\n",
    "    for j in range(i+1,len(feat_needed)):\n",
    "        if dfAll[feat_needed[i]].quantile(0.5) - dfAll[feat_needed[j]].quantile(0.5)<=1 and dfAll[feat_needed[i]].quantile(0.5) - dfAll[feat_needed[j]].quantile(0.5)>=-1:\n",
    "            tmp_var = feat_needed[i]+'-'+feat_needed[j]\n",
    "            dfAll[tmp_var] = dfAll[feat_needed[i]]- dfAll[feat_needed[j]]\n",
    "            gap_list.append(tmp_var)\n",
    "            print(tmp_var)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_needed2 =[]\n",
    "for var in feat_needed:\n",
    "    if dfAll[var].nunique()>15:\n",
    "        feat_needed2.append(var)\n",
    "gap_list = []\n",
    "for i in range(len(feat_needed2)-1):\n",
    "    for j in range(i+1,len(feat_needed2)):\n",
    "        if dfAll[feat_needed2[i]].quantile(0.5) - dfAll[feat_needed2[j]].quantile(0.5)<=2 and dfAll[feat_needed2[i]].quantile(0.5) - dfAll[feat_needed2[j]].quantile(0.5)>=-2:\n",
    "            tmp_var = feat_needed2[i]+'-'+feat_needed2[j]\n",
    "            dfAll[tmp_var] = dfAll[feat_needed2[i]]- dfAll[feat_needed2[j]]\n",
    "            gap_list.append(tmp_var)\n",
    "            print(tmp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfAll = dfAll.set_index(dfAll['ID'])\n",
    "dfTrain_tmp = dfAll.loc[dfTrain['ID']]\n",
    "dfPred_tmp = dfAll.loc[dfPred['ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfPred = dfPred_tmp.copy(deep=True)\n",
    "dfTrain = dfTrain_tmp.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feat_file = ['Feat_cnt_col','Feat_cnt_tool','Feat_pcent_col','Feat_pcent_tool']\n",
    "ex_feat = []\n",
    "for filename in Feat_file:\n",
    "    tmp = pd.read_csv('../../Cache/'+filename+'.csv')\n",
    "    tmp.set_index(tmp['ID'],inplace=True,drop=True)\n",
    "    tmp_feat_list = tmp.columns.tolist()\n",
    "    if 'ID' in tmp_feat_list:\n",
    "        tmp_feat_list.remove('ID')\n",
    "    ex_feat += tmp_feat_list\n",
    "    dfTrain = pd.merge(dfTrain,tmp,'left','ID')\n",
    "    dfPred = pd.merge(dfPred,tmp,'left','ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfAll2 = pd.read_csv('../../Data/All_Data_dedup.csv',nrows=2)\n",
    "raw_predictors = dfAll2.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../../Cache/var_change.txt','r')\n",
    "a = f.read()\n",
    "var_change = eval(a)\n",
    "f.close()\n",
    "predictors = dfTrain.columns.tolist()[1:]+ex_feat\n",
    "feat_cnt = 0\n",
    "for key,value in var_change.items():\n",
    "    feat_cnt+=1\n",
    "    if feat_cnt>1:\n",
    "        continue\n",
    "    for var in var_change[key]['constant']:\n",
    "        try:\n",
    "             predictors.remove(var)\n",
    "        except:\n",
    "            continue\n",
    "    for var in var_change[key]['category']:\n",
    "        try:\n",
    "            predictors.remove(var)\n",
    "        except:\n",
    "            continue\n",
    "        tmpTrain = pd.get_dummies(dfTrain[var],prefix=var,dummy_na=True)\n",
    "        tmpPred = pd.get_dummies(dfPred[var],prefix=var,dummy_na=True)\n",
    "        predictors = predictors + tmpTrain.columns.tolist()\n",
    "        dfTrain = pd.concat([dfTrain,tmpTrain],axis=1)\n",
    "        dfPred = pd.concat([dfPred,tmpPred],axis=1)\n",
    "for var in predictors:\n",
    "    if var not in dfPred.columns:\n",
    "        dfPred[var] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_kfold(dfTrain,dfPred,predictors,n_splits=5, params = {'max_depth':3, 'eta':0.01, 'silent':0,'objective':'reg:linear','lambda':1,'subsample':0.8,\n",
    "                         'colsample_bytree':0.4}):  \n",
    "    kf = KFold(n_splits=n_splits,shuffle=True)\n",
    "    dpred = xgb.DMatrix(dfPred[predictors].values,label=[0]*len(dfPred),missing=np.nan)\n",
    "    imp = pd.DataFrame({'variable':predictors,'lk':['f'+str(i) for i in range(len(predictors))]})\n",
    "    round=0\n",
    "    for train_index, test_index in kf.split(dfTrain):\n",
    "        round+=1\n",
    "        train_X = dfTrain.loc[train_index,predictors]\n",
    "        test_X = dfTrain.loc[test_index,predictors]\n",
    "        train_Y = dfTrain.loc[train_index,'Y']\n",
    "        test_Y = dfTrain.loc[test_index,'Y']\n",
    "\n",
    "        dtrain = xgb.DMatrix(train_X.values, label=train_Y.values, missing = np.nan)\n",
    "        dtest = xgb.DMatrix(test_X.values, label=test_Y.values, missing = np.nan)\n",
    "        param = params \n",
    "        evallist  = [(dtrain,'train'),(dtest,'eval')]  \n",
    "        num_round = 5000\n",
    "        evals_dict = {}\n",
    "        model = xgb.train(param,dtrain,num_round, evallist,early_stopping_rounds=50,evals_result=evals_dict,verbose_eval =1000)\n",
    "        performance_df = pd.DataFrame(evals_dict['eval'])\n",
    "        bst_tree = len(performance_df)-51\n",
    "        pred_test = model.predict(dtest,ntree_limit =bst_tree)\n",
    "\n",
    "        tmp_imp = pd.DataFrame(model.get_score(importance_type='gain'),index=['imp_fold%d'%round]).T\n",
    "        tmp_imp['lk'] = tmp_imp.index\n",
    "        imp = imp.merge(tmp_imp,'left','lk').fillna(0)\n",
    "\n",
    "\n",
    "        pred_score = model.predict(dpred,ntree_limit =bst_tree)\n",
    "        if round==1:\n",
    "            test_result = pd.DataFrame({'ID':dfTrain.loc[test_index,'ID'].values,'score':pred_test,'target':test_Y})\n",
    "            result = pd.DataFrame({'ID':dfPred['ID'],'Score_%d'%round:pred_score})\n",
    "        else:\n",
    "            test_result = pd.concat([test_result,pd.DataFrame({'ID':dfTrain.loc[test_index,'ID'].values,'score':pred_test,'target':test_Y})],axis=0)\n",
    "            result = result.merge(pd.DataFrame({'ID':dfPred['ID'],'Score_%d'%round:pred_score}),'inner','ID')\n",
    "    #print(\"Test MSE:\",metrics.mean_squared_error(test_result['target'], test_result['score']))\n",
    "    return test_result,result,imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain2 = pd.read_csv(config.original_train_data_path,usecols=['ID','Y'])\n",
    "dfTrain = dfTrain.merge(dfTrain2,'inner','ID')\n",
    "dfTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result,result,imp = xgb_kfold(dfTrain,dfPred,predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result['score']=result[['Score_1','Score_2','Score_3','Score_4','Score_5']].mean(axis=1)\n",
    "submit = result[['ID','score']]\n",
    "today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "submit.to_csv('../../Submission/submit_%s.csv'%today,header=False,index=False)\n",
    "imp.to_csv('../../Profiling/imp_test_v3_%s.csv'%today,index=False)\n",
    "test_result('../../Profiling/test_score_%s.csv'%today,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
