{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from nlp_utils import clean_text, pos_tag_text\n",
    "sys.path.append(\"../\")\n",
    "from param_config import config\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option('display.float_format',lambda x: '%.5f'%x)\n",
    "\n",
    "print(\"Load data...\")\n",
    "\n",
    "dfTrain = pd.read_csv(config.original_train_data_path)\n",
    "\n",
    "dfPred= pd.read_csv(config.original_test_data_path)\n",
    "dfPred2= pd.read_csv(config.original_test_data2_path)\n",
    "# number of train/test samples\n",
    "num_train, num_pred = dfTrain.shape[0], dfPred.shape[0]\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "f = open('../../Data/col_name.txt','r')  \n",
    "a = f.read()  \n",
    "col_name = eval(a)  \n",
    "f.close() \n",
    "\n",
    "f = open('../../Data/Procedure.txt','r')  \n",
    "a = f.read()  \n",
    "procedure = eval(a)  \n",
    "f.close() \n",
    "\n",
    "dfTrain = dfTrain.rename(columns=col_name)\n",
    "raw_predictors = dfTrain.columns.tolist()[1:-1]\n",
    "dfPred = dfPred.rename(columns=col_name)\n",
    "dfPred2 = dfPred2.rename(columns=col_name)\n",
    "\n",
    "dfPredAll = pd.concat([dfPred,dfPred2],axis=0)\n",
    "\n",
    "dfAll = pd.concat([dfTrain,dfPred,dfPred2],axis=0)\n",
    "\n",
    "#dfAll = dfAll.replace({'':np.nan,0:np.nan})\n",
    "dfAll = dfAll.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 7171)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#处理0这个比较异常的值,在trian set中有40条数据，在某些column上为0 ，需要处理，test_a中也有一条\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#number of NAN\n",
    "dfAll['Total_NAN_Num']=dfAll[raw_predictors].T.isnull().sum().tolist()\n",
    "for key,pro in procedure.items():\n",
    "    dfAll[key+'_NAN_Num']=dfAll[pro].T.isnull().sum().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#Count 计算特定值在变量中重复出现的次数\n",
    "dfAll = dfAll.replace({np.nan:-99999999})\n",
    "added = ['ID']\n",
    "for var in raw_predictors:\n",
    "    dfAll['cnt_'+var] = 0\n",
    "    added.append('cnt_'+var)\n",
    "    groups = dfAll.groupby([var])\n",
    "    for name,group in groups:\n",
    "        count = group[var].count()\n",
    "        dfAll['cnt_'+var].ix[group.index] = count\n",
    "dfAll[added].to_csv('../../Cache/Feat_cnt_col.csv',index=False)\n",
    "\n",
    "added = ['ID']\n",
    "for tool,pro in procedure.items():\n",
    "    print('%.2f%%'%(float(t)*100/len(raw_predictors)))\n",
    "    if tool[-3:] in ['261','400','420']:\n",
    "        continue\n",
    "    groups = dfAll.groupby(['TOOL_'+tool[-3:]])\n",
    "    for var in pro:\n",
    "        dfAll['cnt_'+tool+'_'+var] = 0\n",
    "        added.append('cnt_'+tool+'_'+var)\n",
    "        for name,group in groups:\n",
    "            grps = group.groupby([var])\n",
    "            for name2,grp in grps:\n",
    "                dfAll['cnt_'+tool+'_'+var].ix[grp.index] = float(len(grp))/float(len(group))\n",
    "dfAll[added].to_csv('../../Cache/Feat_cnt_tool.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#percentile 计算某一个值在全集中的位置（百分制）\n",
    "dfAll = dfAll.replace({:-99999999:np.nan})\n",
    "added = ['ID']\n",
    "for var in raw_predictors:\n",
    "    if 'TOOL' not in var:\n",
    "        dfAll['pcent_'+var] = dfAll[var].rank(method='max')/float(len(dfAll))\n",
    "        added.append('pcent_'+var)\n",
    "dfAll[added].to_csv('../../Data/Feat_pcent_col.csv',index=False)\n",
    "t = 0\n",
    "added = ['ID']\n",
    "for tool,pro in procedure.items():\n",
    "    if tool[-3:] in ['261','400','420']:\n",
    "        continue\n",
    "    groups = dfAll.groupby(['TOOL_'+tool[-3:]]) \n",
    "    for var in pro:\n",
    "        dfAll['pcent_'+tool+'_'+var] = 0\n",
    "        added.append('pcent_'+tool+'_'+var)\n",
    "            for name,group in groups:\n",
    "            dfAll['pcent_'+tool+'_'+var].ix[group.index] = group[var].rank(method='max')/float(len(group))\n",
    "            \n",
    "dfAll[added].to_csv('../../Data/Feat_pcent_tool.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pivot 创建多组category variable 的两两交叉表,感觉只能用来计算分类型变量,好像现在直接可以用pandas解决\n",
    "def get_pivottable(X_train, X_test, use='all',Feat_list):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries, one per feature in the\n",
    "    basic data, containing cross-tabulated counts\n",
    "    for each column and each value of the feature.\n",
    "    \"\"\"\n",
    "    dictionaries = []\n",
    "    if use == 'all':\n",
    "        X = np.vstack((X_train, X_test))\n",
    "        filename = \"pivottable\"\n",
    "    elif use == 'train':\n",
    "        X = X_train\n",
    "        filename = \"pivottable_train\"\n",
    "    else:\n",
    "        X = X_test\n",
    "        filename = \"pivottable_test\"\n",
    "    for i in range(len(Feat_list)):\n",
    "        dictionaries.append({'total': 0})\n",
    "    try:\n",
    "        with open(\"cache/%s.pkl\" % filename, 'rb') as f:\n",
    "            logger.debug(\"loading cross-tabulated data from cache\")\n",
    "            dictionaries = pickle.load(f)\n",
    "    except IOError:\n",
    "        logger.debug(\"no cache found, cross-tabulating data\")\n",
    "        for i, row in enumerate(X):\n",
    "            for j in range(len(Feat_list)):\n",
    "                dictionaries[j]['total'] += 1\n",
    "                if row[j] not in dictionaries[j]:\n",
    "                    dictionaries[j][row[j]] = {'total': 1}\n",
    "                    for k, key in enumerate(Feat_list):\n",
    "                        dictionaries[j][row[j]][key] = {row[k]: 1}\n",
    "                else:\n",
    "                    dictionaries[j][row[j]]['total'] += 1\n",
    "                    for k, key in enumerate(Feat_list):\n",
    "                        if row[k] not in dictionaries[j][row[j]][key]:\n",
    "                            dictionaries[j][row[j]][key][row[k]] = 1\n",
    "                        else:\n",
    "                            dictionaries[j][row[j]][key][row[k]] += 1\n",
    "        with open(\"cache/%s.pkl\" % filename, 'wb') as f:\n",
    "            pickle.dump(dictionaries, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return dictionaries\n",
    "\n",
    "def creat_cate_corr(X_train, X_test, Feat_list=[var if 'TOOL' in var for var in raw_predictor]):\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "    dictionaries = get_pivottable(X_train[Feat_list], X_test[Feat_list],Feat_list=Feat_list)\n",
    "    #dictionaries_train = get_pivottable(X_train[Feat_list], X_test[Feat_list], use='train',Feat_list=Feat_list)\n",
    "    #dictionaries_test = get_pivottable(X_test[Feat_list], X_test[Feat_list], use='test',Feat_list=Feat_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
